## 제5장. 안정 해시 설계 (Consistent Hashing)

> 서버가 수시로 늘고 줄어드는 환경에서도 전체 키를 재배치하지 않고 영향 범위를 `k/n` 수준으로 제한하는 분산 해시 기법

### 5.1 왜 안정 해시인가?
- **고전적 해시 한계**: `hash(key) % N` 은 N 변동 시 거의 모든 키가 이동 → 캐시 미스, 네트워크 트래픽 폭증
- **안정 해시 정의**: 평균적으로 전체 키 중 `k/n` 개만 재배치되도록 고안된 해시 기법
- **대표 활용 사례**: Dynamo, Cassandra, Akamai CDN, Consistent Hash 기반 L4/L7 로드 밸런서

### 5.2 해시 공간과 링 모델
1. **해시 공간(Hash Space)**: SHA-1, MurmurHash 등 160bit 해시 출력 범위(`0 ~ 2^160-1`)
2. **해시 링(Hash Ring)**: 해시 공간의 양 끝을 연결한 원형 좌표계
3. **노드/키 배치**
   - 서버 식별자(IP, 토큰 등)에 해시를 적용해 링 위 위치 결정
   - 키 또한 같은 해시 함수로 좌표 계산
   - 키 위치에서 **시계 방향으로 최초로 만나는 서버**가 책임 노드

```
[해시 링 개념도]
키 위치 ──▶ (시계방향) ──▶ 최초 서버가 키 저장
```

### 5.3 요청 처리 흐름
1. 클라이언트가 `put(key, value)` 요청
2. 조정자가 키 해시 계산 후 책임 노드를 선정
3. 다중화 설정 시 링을 따라 `N`개의 후속 노드에 복제
4. 읽기 시 `R`개의 응답을 모아 최신 버전을 결정(쿼럼)

### 5.4 구현 시 주요 이슈와 해결책

| 문제 | 증상 | 해결 전략 |
| --- | --- | --- |
| 파티션 크기 불균등 | 특정 노드가 과도한 구간 담당 → 핫스팟 | 가상 노드(VNode) 추가, 토큰 자동 재조정, 주기적 리밸런싱 |
| 키 분포 불균등 | 해시 편향/키 패턴으로 구간 집중 | 강한 해시(Murmur, Jump Hash), 키 prefix 샤딩, VNode 수 확장 |

#### 가상 노드(VNode) 운용 팁
- 물리 서버 하나가 수십~수백 개 토큰을 소유
- 장애 시 VNode 단위로 위임(handoff)하여 빠르게 책임 전환 가능
- 토큰 수를 늘려 데이터 이동량을 세분화하고 throttle 제어

#### 운영 체크리스트
- **토큰 자동화**: 신규 노드 합류 시 토큰 계산·배포 자동화
- **모니터링**: 노드별 파티션 크기, 키 개수, 데이터 이동 속도
- **데이터 재배치 전략**: 백그라운드 스레드, 외부 버퍼(S3 등) 활용해 서비스 영향 최소화

---

## 제6장. 키-값 저장소 설계 (Key-Value Store)

> **핵심 메시지**  
> 완전 분산형 아키텍처 위에서 CAP 트레이드오프를 선택하고, `N/R/W` 파라미터로 일관성 수준을 튜닝하며, 버전 관리로 충돌을 해소한다.

### 6.1 CAP 정리와 시스템 유형

| 유형 | 보장 | 포기 | 예시 |
| --- | --- | --- | --- |
| **CP** | 일관성 + 파티션 감내 | 가용성 | HBase, ZooKeeper |
| **AP** | 가용성 + 파티션 감내 | 즉시 일관성 | Cassandra, Dynamo |
| **CA** | (이론상) 일관성 + 가용성 | 파티션 감내 | 단일 머신 RDB |

- 현실적 분산 시스템은 **파티션 감내(P)** 가 전제 → CP 또는 AP 중 선택

### 6.2 튜너블 일관성: N, W, R
- `N`: 복제본 수, `W`: 쓰기 성공으로 간주할 응답 수, `R`: 읽기 성공 응답 수
- **강한 일관성 조건**: `W + R > N`
- **지연 최소화**: `W = 1`, `R = 1` (최신성은 희생)

| 워크로드 | 설정 예시 | 특징 |
| --- | --- | --- |
| 읽기 위주 | `N=3, W=1, R=2` | 쓰기 빠름, 읽기는 쿼럼으로 최신성 확보 |
| 쓰기 위주 | `N=5, W=3, R=1` | 쓰기 안정성 우선, 읽기 지연 최소화 |
| 강한 일관성 근접 | `N=3, W=2, R=2` | 지연 증가 대신 최신 데이터 제공 |

### 6.3 비일관성 해소 기법
- **데이터 버전 관리**: Immutable 버전 저장, 롤백 및 비교 용이
- **벡터 시계(Vector Clock)**: `(노드ID, 카운터)` 집합으로 causality 추적 → 병합 기준 제공
- **Hinted Handoff**: 장애 노드 대신 이웃 노드가 임시 저장 후 복구 시 전달
- **Read Repair**: 읽기 요청 시 오래된 복제본을 백그라운드로 갱신
- **Merkle Tree Anti-Entropy**: 차이점만 동기화해 네트워크 비용 감소
- **CRDT/연산 기반 병합**: 실시간 협업·카운터 등에서 자동 병합 제공

### 6.4 시스템 아키텍처와 요청 흐름
1. **클라이언트**: `get`, `put`, `delete` API 호출
2. **조정자(Coordinator)**: 책임 노드 집합 도출, `R/W` 쿼럼 추적
3. **노드(Node)**: 안정 해시 링 위에 분포, 저장 엔진 + 메타데이터 + 가십 참여
4. **저장 엔진**: LSM-Tree(SSTable)·B-Tree 등, 컴팩션·압축 전략 포함
5. **가십 프로토콜**: 멤버십/하트비트 공유로 장애 감지 및 뷰 일치

#### 요청 시퀀스
1. 클라이언트 → 조정자: `put(key, value)`
2. 조정자 → 링 탐색: 책임 노드 목록 산출
3. 병렬 쓰기 및 ACK 수집 (`W` 달성 시 성공)
4. 읽기 시 다중 버전 병합 → 최신 데이터 응답 → 필요 시 read repair

### 6.5 운영 인사이트
- **데이터 모델링**: 파티션 키 설계가 확장성 결정 → 균등한 해시 유도
- **SLA 기반 튜닝**: 지연·RPO/RTO 요구에 맞춰 `N/R/W` 재조정
- **백업/DR 전략**: 스냅샷 + 증분 로그, 크로스 리전 복제, TTL 기반 정리
- **관측성**: 쿼럼 지연, 시간 동기화(노드 시계), 힌트 누적량, 토큰 가용성 모니터링