# 5장 — 안정 해시(Consistent Hashing) 정리

분산 시스템에서 서버 수가 변할 때 전체 데이터를 다시 배치해야 하는 문제를 해결하기 위해 등장한 기술이 **안정 해시(Consistent Hashing)** 이다.
이 문서는 안정 해시의 기본 개념부터 해시 링, 서버 배치, 키 매핑, 가상 노드, 재배치 범위까지 전체 흐름을 정리한다.

---

## 1. 기존 해시 방식의 한계

일반적인 분산 캐시 분배 공식은 다음과 같다.

```text
serverIndex = hash(key) % N
```

문제는 **서버 수 N이 변하면 모든 key의 서버 인덱스가 바뀐다는 것**이다.

예:

* 서버 4개 → %4
* 서버 3개로 줄어들면 → %3
  → 대부분의 key가 다른 서버로 이동
  → **대규모 캐시 미스 발생**

그래서 더 나은 방식이 필요하다.

---

## 2. 안정 해시란?

> 안정 해시(Consistent Hashing)는 **서버 수가 변해도 전체 키 중 극히 일부만 재배치되는** 해싱 기법이다.

특징:

* 평균적으로 O(k/n)의 키만 재배치되며,
* 서버 변화가 있어도 전체 시스템에 영향이 최소화된다.

---

## 3. 해시 공간과 해시 링(Hash Ring)

SHA-1과 같은 해시 함수의 출력 범위를
`0 ~ 2^160-1` 범위의 선형 공간이라 할 때,

이를 **양 끝을 연결하여 원형으로 표현**한다.
이것이 **해시 링(Hash Ring)** 이다.

```
0 → 1 → 2 → ... → MAX → 다시 0 (순환)
```

이 링 위에 서버와 키가 위치한다.

---

## 4. 서버를 해시 링에 배치하기

각 서버의 IP 또는 이름을 해싱하여 링의 특정 위치에 매핑한다.

예시:

* s0 = 서버0
* s1 = 서버1
* s2 = 서버2
* s3 = 서버3

각 서버는 링 위에 특정 지점 하나를 차지한다.

---

## 5. 키를 해시 링에 배치하고 서버 찾기

키(key0, key1…)도 동일한 방식으로 링 위에 배치한다.

키가 어느 서버에 저장되는지는 다음 규칙으로 정한다:

### **📌 키의 위치에서 시계 방향으로 이동했을 때 처음 만나는 서버가 담당 서버이다.**

예:

* key0 → 시계 방향 첫 서버 s0
* key1 → s1
* key2, key3 → s3

---

## 6. 서버 추가/삭제 시 재배치

안정 해시의 핵심 장점: **일부 키만 움직인다.**

### ✅ 서버 추가 시

* 새 서버 s_new 위치에서
* **반시계 방향으로 돌면 처음 만나는 서버까지의 키만 재배치**

예: s4 추가

* s3와 s4 사이의 키만 s4로 이동

### ✅ 서버 삭제 시

* 삭제된 서버 s_dead가 갖고 있던 키만
* **시계방향 첫 서버로 이동**

예: s1 삭제

* s1과 s2 사이 키들이 s2로 이동

➡ 전체 재배치가 아니라 "인접한 구간만" 이동하는 것이 핵심

---

## 7. 기본 안정 해시의 두 가지 문제

서버를 1회만 링에 배치하면 다음 문제가 생긴다.

### 문제 1) 파티션 크기 불균형

서버 위치가 랜덤이기 때문에
어떤 서버는 큰 공간을 맡고 어떤 서버는 작은 공간만 맡음

### 문제 2) 키의 균등 분포 실패

서버 간 간격이 고르지 않아 키가 특정 서버로 몰림

➡ 이를 해결하기 위해 **가상 노드(Virtual Node)** 개념이 등장

---

## 8. 가상 노드(Virtual Node) 도입

가상 노드는 다음 개념이다.

> **실제 서버를 해시 링에 여러 번 복제해서 배치한다.**

예:

* 서버0 → s0_0, s0_1, s0_2
* 서버1 → s1_0, s1_1, s1_2

이렇게 하면:

* 서버가 링 전체에 골고루 흩어짐
* 파티션이 균등해짐
* 키 분포 또한 균형적으로 배치됨

### 키의 저장 기준도 동일

* 키 → 시계 방향 → 처음 만나는 가상 노드
* 그 가상 노드가 속한 실제 서버가 key 저장

---

## 9. 가상 노드의 효과와 트레이드오프

### ✔ 효과 1: 파티션 균등화

가상 노드 수가 많아질수록 파티션 불균형이 감소

### ✔ 효과 2: 키 분포 개선

표준편차가 작아져 키가 고르게 분포됨
(예: 100~200개의 가상 노드 사용 시 표준편차는 평균의 약 5~10%)

### ✔ 효과 3: 서버 추가/삭제 시 안정적

이동되는 키는 가상 노드 단위로 최소화됨

### ⚠ 단점: 관리 비용 증가

가상 노드 수가 많아질수록

* 메모리 사용 증가
* 해시 탐색 계산량 증가

➡ 시스템 사양에 맞게 적절한 가상 노드 수 선정 필요

---

## 전체 흐름 요약

1. 전체 해시 공간을 원형으로 만든다 (해시 링)
2. 서버를 해싱하여 링에 배치
3. 키를 해싱하여 링에 배치
4. 키는 시계 방향 첫 서버가 담당
5. 서버 추가/삭제 시 인접한 범위 키만 이동
6. 가상 노드를 사용해 부하 균형을 맞춤

---

## 결론

안정 해시는 대규모 분산 시스템에서 서버 수 변동에도
데이터 이동량을 최소화하며 부하를 안정적으로 분배할 수 있는 핵심 기법이다.

**Memcached, Redis Cluster, Cassandra, DynamoDB 등**
다양한 시스템이 이 기법을 기반으로 동작한다.

---

# 6장 - 분산 키-값 저장소 정리

## 1. 분산 키-값 저장소 개요

분산 키-값 저장소(distributed key-value store)는 데이터를 여러 서버에 분산 저장하여 **높은 가용성**, **확장성**, **장애 내성**을 확보하기 위한 저장소 구조이다.
이를 설계할 때 가장 중요한 이론은 **CAP 정리**이다.

---

## 2. CAP 정리

CAP 정리는 분산 시스템에서 **Consistency(일관성)**, **Availability(가용성)**, **Partition Tolerance(파티션 감내)** 중 **동시에 두 가지만** 선택할 수 있다는 원리이다.

### 2.1 세 가지 요소

| 요소             | 의미                             |
| -------------- | ------------------------------ |
| **일관성 (C)**    | 모든 클라이언트가 같은 시점에서 동일한 데이터를 읽는다 |
| **가용성 (A)**    | 일부 노드가 장애가 나도 시스템이 계속 응답한다     |
| **파티션 감내 (P)** | 네트워크가 분리(파티션)되더라도 시스템이 지속 동작한다 |

### 2.2 조합별 시스템 유형

| 유형         | 선택되는 요소                            | 희생되는 요소             | 특징                               |
| ---------- | ---------------------------------- | ------------------- | -------------------------------- |
| **CP 시스템** | Consistency + Partition Tolerance  | Availability        | 정확한 데이터가 중요 (예: 은행)              |
| **AP 시스템** | Availability + Partition Tolerance | Consistency         | 항상 응답해야 하는 서비스 (예: SNS timeline) |
| **CA 시스템** | Consistency + Availability         | Partition tolerance | 이론적으로만 존재(분산 시스템에서는 P 필수)        |

---

## 3. 분산 시스템에서의 파티션과 실제 동작

### 3.1 이상적 환경

* 네트워크 파티션이 절대 발생하지 않음
* 모든 데이터는 즉시 복제됨 → C + A 충족

### 3.2 실제 환경

* 파티션은 항상 발생할 수 있음
* 이때 **일관성 vs 가용성** 중 하나를 선택해야 한다
  예: 어떤 노드는 최신 데이터를 가지고 있지 않을 수 있음

---

## 4. 분산 키-값 저장소의 핵심 컴포넌트

분산 저장소를 구성하는 핵심 기술 요소는 다음과 같다.

1. 데이터 파티션 (partitioning)
2. 데이터 다중화 (replication)
3. 일관성 (consistency)
4. 일관성 불일치 해소 (inconsistency resolution)
5. 장애 처리 (failure handling)
6. 시스템 아키텍처 다이어그램
7. 쓰기 경로 (write path)
8. 읽기 경로 (read path)

이 문서에서는 Dynamo, Cassandra, BigTable의 구조를 참고한다.

---

## 5. 데이터 파티션

### 5.1 파티션 목표

파티션은 데이터를 여러 서버로 분산 저장하기 위한 기술이다. 두 가지 목표:

* 데이터를 여러 서버에 골고루 분산
* 서버 추가/삭제 시 데이터 이동 최소화

### 5.2 일관 해시(Consistent Hashing)

* 서버를 링 구조에 배치
* 키를 해싱하여 링에서 **시계 방향으로 가장 먼저 만나는 노드**에 저장
* 서버 추가/삭제 시 데이터 이동량 최소화

#### 장점

* 자동 확장(autoscaling)
* 노드 수에 비례하는 부하 분산
* 가상 노드(virtual node)로 균등성 확보

---

## 6. 데이터 다중화 (Replication)

### 6.1 목적

* 높은 가용성
* 장애 발생 시 데이터 손실 방지

### 6.2 방법

* 하나의 데이터는 **N개의 노드에 복제**
* 일관 해시로 배치된 첫 N개 노드가 복제본을 저장

📌 단, 실제 서버 수가 부족하면 선택된 노드가 같은 물리 서버일 수 있으므로
**물리 서버 중복 회피** 설계 필요.

---

## 7. 데이터 일관성 모델

일관성을 보장하기 위한 요소는:

* N = 복제본 수
* W = 쓰기에서 성공 판단을 위한 응답 수
* R = 읽기에서 성공 판단을 위한 응답 수

### 7.1 조합별 의미

| 구성        | 의미       |
| --------- | -------- |
| R=1, W=N  | 빠른 읽기    |
| W=1, R=N  | 빠른 쓰기    |
| W + R > N | 강한 일관성   |
| W + R ≤ N | 일관성 보장 X |

---

## 8. 일관성 모델 (Consistency Model)

| 모델                       | 설명                      |
| ------------------------ | ----------------------- |
| **Strong Consistency**   | 항상 최신 데이터 반환            |
| **Weak Consistency**     | 최신 데이터 보장 안 함           |
| **Eventual Consistency** | 시간이 지나면 결국 모든 복제본이 동기화됨 |

대부분의 분산 저장소는 **결과적 일관성(Eventual Consistency)**을 택한다.

---

## 9. 비일관성 해소: 데이터 버저닝 (Versioning)

버저닝은 변경 시 새로운 버전을 생성하는 방식이다.
대표 기술: **벡터 시계(Vector Clock)**

### 9.1 충돌 발생 예시

* 서버1: name = "johnSanFrancisco"
* 서버2: name = "johnNewYork"

동기화되면 서로 공존하는 버전(v1, v2)을 가지게 됨 → **충돌(conflict)**

### 9.2 벡터 시계

* 각 노드가 자신이 처리한 업데이트를 버전 카운터로 기록
* 버전 간 선후관계 판단 가능
* 충돌 해결 시 활용

---

## 10. 장애 처리

### 10.1 장애 감지 (Failure Detection)

#### 방식 1: Multicasting

비효율적 (노드가 많으면 비용 증가)

#### 방식 2: Gossip Protocol

효율적인 분산 장애 감지 방법

동작 원리:

* 각 노드는 membership list 유지
* heartbeat counter 주기적 증가
* 다른 노드에 주기적으로 전달
* 변경되지 않는 노드는 장애로 판단

---

## 11. 장애 처리 기법

### 11.1 일시적 장애 처리 (Sloppy Quorum + Hinted Handoff)

* P(파티션 감내), A(가용성) 우선
* 쓰기 요청을 정상 노드에 임시 저장
* 장애 노드 복구 후 해당 데이터 전달

### 11.2 영구 장애 처리 (Anti-Entropy + Merkle Tree)

* 각 노드 간 데이터 동기화
* Merkle Tree를 사용하여 데이터 차이를 최소 비용으로 비교
* 대규모 데이터 동기화에 효율적

---

## 12. 데이터 센터 장애 처리

* 여러 데이터 센터에 복제하여 하나의 센터가 다운되어도 서비스 지속
* 센터 간 고속 네트워크 필요
* 지리적 이슈를 고려한 다중화 전략 필요

---

## 13. 시스템 아키텍처

### 13.1 주요 구성 요소

* 클라이언트: get(), put() 요청
* 중재자(Coordinator): 요청을 받아 적절한 노드에 전달
* 노드: 해시링에 분포, 데이터 저장

### 13.2 특징

* 완전 분산 구조(decentralized)
* SPOF 없음
* 모든 노드는 동일한 역할을 수행

---

## 14. 쓰기 경로 (Write Path)

1. 커밋 로그(Commit Log)에 기록
2. 메모리 캐시(Memtable)에 반영
3. 일정 기준에 도달하면 디스크에 SSTable로 flush

---

## 15. 읽기 경로 (Read Path)

1. 메모리 캐시에서 조회
2. 없으면 Bloom Filter로 후보 SSTable 검색
3. 해당 SSTable에서 조회
4. 결과 반환

Bloom Filter는 **“키가 없다는 것”을 빠르게 판별**하는 데 사용됨.

---

## 16. 요약

이 문서에서 정리한 것은 분산 Key-Value Store 설계의 필수 요소로 다음을 포함한다.

* CAP 정리와 선택의 기준
* 일관 해시와 데이터 분산
* 복제 및 일관성 모델
* 충돌 해결(버저닝, 벡터 시계)
* 장애 감지(gossip) 및 장애 처리(hinted handoff, merkle tree)
* 읽기/쓰기 경로(Cassandra 기반)

---



