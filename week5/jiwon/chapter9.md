# **크롤러란 ?**

---

- 웹 크롤러는 검색 엔진에서 널리 쓰는 기술로, 웹에 새로 올라오거나 갱신된 콘텐츠를 찾아내는 것을 목적으로 한다.

# **크롤러의 쓰임**

---

- **검색 엔진 인덱싱 :** 크롤러는 웹 페이지를 모아 검색 엔진을 위한 로컬 인덱스를 만든다.
- **웹 아카이빙 :** 추후 사용을 목적으로 장기 보관하기 위해 웹에서 정보를 모으는 것이다.
- **웹 마이닝 :** 인터넷을 통해 유의미한 자료를 추출해 내는 것이다.
- **웹 모니터링 :** 저작권이나 상표권 침해 사례를 모니터링하는 것이다.

## 1. 문제 이해 및 설계 범위 확정

웹 크롤러의 기본 알고리즘은 다음과 같다.

> 1. URL 집합이 입력으로 주어지면, 해당 URL들이 가리키는 모든 웹 페이지를 다운로드한다.
>

기본 알고리즘을 바탕으로 많은 양과 다양한 데이터를 지원할 수 있는 확장성과 저장용량, 장애나 악성 코드와 같은 비정상적인 입력과 웹 페이지에 대한 안정성, 크롤링하고자 하는 페이지에 너무 많은 요청을 보내지 않도록 하는 등 다양한 사항을 고려하여 설계한다.

## 2. 개략적 설계안 제시 및 동의 구하기

웹 크롤러에 대한 선행연구를 참고하여 설계한 각 컴포넌트들의 기능과 작업 흐름을 살펴본다.

> - 시작 URL 집합
>
> - 미수집 URL 저장소
> - HTML 다운로더
> - 도메인 이름 변환기
> - 콘텐츠 파서
> - 중복 콘텐츠 확인
> - 콘텐츠 저장소
> - URL 추출기
> - URL 필터
> - 중복 URL 확인
> - URL 저장소

웹 크롤러는 위의 컴포넌트들을 순서대로 거치며 크롤링을 수행한다.

## 3. 상세 설계

### 탐색 알고리즘

웹은 링크로 연결된 일종의 그래프이다. 그렇기 때문에 DFS, BFS와 같은 그래프 탐색 알고리즘을 사용하여 크롤링을 수행할 수 있다. 일반적으로는 그래프의 크기가 클 경우 얼마나 깊이 탐색하게 될지 모르기 떄문에 DFS보다는 BFS를 사용한다.

BFS를 사용할 때에는 다음의 두가지 문제가 발생한다. 첫번쨰로 한 페이지에 연결된 대부분의 링크는 같은 서버의 특정 위치로 연결된다. 이때문에 크롤러는 같은 호스트에 대한 링크들을 반복적으로 다운로드받아서 서버에 부하를 줄 수 있다.

두번째로는 URL간에 우선순위가 없다는 것이다. 웹 사이트는 페이지 순위, 사용자 트래픽의 양, 업데이트 빈도 등 여러가지 척도를 기반으로 우선순위를 구별할 수 있어야 한다.

### 미수집 URL 저장소

미수집 URL 저장소를 이용하여 위의 문제들을 해결할 수 있다.

크롤링의 예의를 지키기 위해서는 동일한 웹 사이트에 대해서 한번에 한 페이지만 요청하는 것이다. 이를 위해서는 웹사이트의 호스트명과 페이지 다운로드 작업 스레드 사이의 관계를 유지하여 처리할 수 있다. 동일한 호스트에 대한 요청은 동일한 큐에 저장하고 큐 선택기가 큐들을 돌아가면서 URL을 추출하여 작업 스레드에 할당한다. 이를 통해서 동일한 호스트에 대한 요청 시간을 조절할 수 있다.

우선순위를 구별하기 위해서는 우선순위를 위한 순위 결정장치와 우선순위별 큐, 그리고 우선순위가 높은 큐에서 더 자주 작업을 가져오도록 구현된 큐 선택기가 필요하다. 이를 기반으로 우선순위가 높은 URL들을 더 먼저 처리하도록 할 수 있다.

이외에도 웹 페이지의 변경 이력과 우선순위 등을 기반으로 중요한 페이지를 갱신함으로 페이지에 대한 신선도를 유지하기 위한 전략을 구성할 수 있다. 또한 많은 URL을 처리하게되면 용량이 커지기 때문에 메모리에 대한 고민을 해야하는데, 메모리 버퍼에 큐를 두고 주기적으로 버퍼의 데이터를 디스크에 저장하도록 하여 성능과 안정성, 규모 확장성 등을 개선할 수 있다.

### HTML 다운로더

HTML 다운로더는 HTTP 프로토콜을 통해 웹 페이지를 내려 받는다.

크롤링을 하기에 앞서서 로봇 제외 프로토콜에 대해서 알아야 한다. 웹 사이트에는 Robots.txt라는 파일을 통해서 크롤러가 수집해도 되는 또는 되지않는 페이지 목록을 나열하고 있다. 크롤링을 하기전에 해당 파일의 내용을 먼저 확인해야한다.

HTML 다운로더에서 작업을 수행할 때 성능을 최적화 할 수 있는 기법들을 다음과 같다.

> - 분산 크롤링: 크롤링 작업을 여러 서버에 분산하여 수행한다.
>

성능외에도 안정 해시를 이용한 부하 분산, 크롤링 상태 및 수집 데이터 저장, 예외처리, 데이터 검증 등을 통해 안정성을 확보할 수 있다. 또한 새로운 콘텐츠들에 대응하여 모듈을 추가하는 등 확장성에 대해서도 고민할 수 있다.

크롤링을 운영할 때는 중복 콘텐츠 , 반복적인 구조를 통한 크롤링 함정, 스팸과 같은 데이터 노이즈 등의 문제를 마주칠 수 있는데, 이러한 URL들에 대해서 대처할 수 있도록 URL 필터를 구현하는 것도 중요하다.