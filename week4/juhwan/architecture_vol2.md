# 제7장. 분산 시스템을 위한 유일 ID 생성기 설계

> **핵심 메시지**  
> 분산 환경에서는 단일 DB의 auto_increment로는 확장이 어렵다. 스노플레이크 방식은 64비트 ID를 타임스탬프·데이터센터·서버·시퀀스로 분할해 유일성과 시간순 정렬을 동시에 달성한다.

---

## 7.1 문제 정의 및 요구사항

### 7.1.1 왜 분산 ID 생성기가 필요한가?

- **단일 DB의 한계**: `auto_increment`는 여러 DB 서버 환경에서 지연 시간 증가 및 동기화 문제 발생
- **분산 환경의 특성**: 여러 서버가 독립적으로 ID를 생성해야 하며, 전역 유일성 보장 필요

### 7.1.2 유일 ID 요구사항

| 요구사항 | 설명 | 중요도 |
| --- | --- | --- |
| **유일성 (Unique)** | 전역적으로 유일성이 보장되어야 함 | 필수 |
| **숫자 구성** | ID는 숫자(numeric)로만 구성 | 필수 |
| **64비트 표현** | 64비트로 표현 가능한 값 | 필수 |
| **정렬 가능 (Sortable)** | 발급 날짜에 따라 정렬 가능 (시간순 정렬) | 필수 |
| **규모** | 초당 10,000개의 ID 생성 가능 | 필수 |
| **고가용성** | 필수 불가결한 컴포넌트이므로 높은 가용성 제공 | 필수 |

---

## 7.2 유일 ID 생성 접근 방식 비교

분산 시스템에서 유일성이 보장되는 ID를 만드는 네 가지 주요 방식을 비교합니다.

| 접근 방식 | 장점 | 단점 | 적합한 경우 |
| --- | --- | --- | --- |
| **다중 마스터 복제** | DB의 auto_increment 활용, 구현 간단 | 데이터센터 간 확장 어려움, 시간순 정렬 보장 불가 | 소규모, 단일 데이터센터 |
| **UUID** | 구현 단순, 서버 간 조율 불필요, 확장 용이 | 128비트로 길고, 시간순 정렬 불가, 64비트 요구사항 미충족 | 유일성만 필요한 경우 |
| **티켓 서버** | 유일성 보장, 숫자 구성, 구현 쉬움 | **단일 실패 지점(SPOF)**, 서버 장애 시 전체 영향 | 중소 규모 애플리케이션 |
| **트위터 스노플레이크** | 모든 요구사항 만족, 분산 확장 가능 | 시스템 설계 복잡도 증가 | 대규모 분산 시스템 |

**결론**: 스노플레이크 방식이 모든 요구사항을 만족하며 분산 환경에 가장 적합합니다.

---

## 7.3 트위터 스노플레이크 상세 설계

### 7.3.1 64비트 ID 구조

스노플레이크는 64비트 ID를 여러 **절(section)**로 분할하여 사용합니다.

```
┌─────────────────────────────────────────────────────────────┐
│                     64비트 ID 구조                            │
├──────┬──────────────┬──────────┬──────────┬────────────────┤
│ Sign │  Timestamp   │ Datacenter│  Server  │   Sequence     │
│ 1bit │    41bit     │   5bit    │   5bit   │     12bit      │
└──────┴──────────────┴──────────┴──────────┴────────────────┘
```

| 절 (Section) | 할당 비트 | 역할 | 범위/용량 |
| --- | --- | --- | --- |
| **사인 비트** | 1비트 | 음수/양수 구분용 (현재 미사용) | 0 또는 1 |
| **타임스탬프** | 41비트 | 기원 시각(epoch) 이후 경과 밀리초 | 약 69년 (2^41 ms) |
| **데이터센터 ID** | 5비트 | 데이터센터 식별자 | 32개 (2^5) |
| **서버 ID** | 5비트 | 데이터센터 내 서버 식별자 | 데이터센터당 32개 (2^5) |
| **일련 번호** | 12비트 | 밀리초당 시퀀스 카운터 | 4,096개/밀리초 (2^12) |

### 7.3.2 각 절의 상세 설명

#### 타임스탬프 (41비트)
- **역할**: ID를 시간순으로 정렬 가능하게 만드는 핵심
- **범위**: 기원 시각(epoch, 예: 2020-01-01 00:00:00 UTC) 이후 경과한 밀리초
- **지속 기간**: 약 69년 (2^41 밀리초 ≈ 69.7년)
- **예시**: 2020-01-01부터 시작하면 2089년까지 사용 가능

#### 데이터센터 ID (5비트)
- **역할**: 여러 데이터센터 환경에서 유일성 보장
- **범위**: 최대 32개 데이터센터 지원
- **특징**: 시스템 시작 시 결정, 운영 중 변경 불가

#### 서버 ID (5비트)
- **역할**: 데이터센터 내 서버 식별
- **범위**: 데이터센터당 최대 32개 서버
- **특징**: 시스템 시작 시 결정, 운영 중 변경 불가

#### 일련 번호 (12비트)
- **역할**: 같은 밀리초 내에서 여러 ID 생성 시 충돌 방지
- **범위**: 밀리초당 최대 4,096개 (2^12)
- **특징**: **1밀리초가 경과할 때마다 0으로 초기화**
- **성능**: 이론상 초당 최대 4,096,000개 ID 생성 가능 (실제로는 타임스탬프와 조합)

### 7.3.3 ID 생성 예시

```
예시: ID = 1234567890123456789 (64비트)

분해:
- 타임스탬프: 1234567890 (밀리초)
- 데이터센터 ID: 1 (5비트)
- 서버 ID: 2 (5비트)
- 시퀀스: 123 (12비트)

→ 이 ID는 2020-01-01 이후 약 14일 경과 시점에
  데이터센터 1, 서버 2에서 생성된 123번째 ID
```

### 7.3.4 성능 계산

- **이론적 최대 처리량**: 
  - 시퀀스 4,096개/밀리초 × 1,000밀리초 = **초당 4,096,000개**
  - 요구사항(초당 10,000개)을 충분히 만족

- **실제 처리량**:
  - 서버당 초당 약 4,096개 (시퀀스 한계)
  - 32개 서버 × 4,096 = 초당 약 131,072개 가능

---

## 7.4 스노플레이크 구현 고려사항

### 7.4.1 시계 동기화 (Clock Synchronization)

- **문제**: 서버 간 시계가 다르면 타임스탬프 기반 정렬이 깨질 수 있음
- **해결**: NTP(Network Time Protocol)를 통한 시계 동기화 필수

### 7.4.2 시퀀스 오버플로우 처리

- **문제**: 같은 밀리초에 4,096개 이상의 ID 요청 시
- **해결**: 
  - 다음 밀리초까지 대기
  - 또는 타임스탬프를 1밀리초 증가시켜 처리

### 7.4.3 장애 처리

- **데이터센터/서버 ID 관리**: 설정 파일 또는 분산 코디네이터(ZooKeeper)로 관리
- **고가용성**: 여러 ID 생성 서버를 운영하여 장애 시 자동 전환

---

## 7.5 스노플레이크의 장단점

### 장점
- ✅ 모든 요구사항 만족 (유일성, 64비트, 시간순 정렬, 고성능)
- ✅ 분산 환경에서 확장 가능
- ✅ 서버 간 동기화 불필요
- ✅ 시간순 정렬로 인덱싱 최적화 용이

### 단점
- ❌ 시스템 설계 복잡도 증가
- ❌ 시계 동기화 의존성
- ❌ 데이터센터/서버 ID 관리 필요

---

# 제8장. URL 단축기 설계

> **핵심 메시지**  
> 긴 URL을 짧은 URL로 변환하는 서비스. Base-62 인코딩으로 숫자 ID를 7자리 문자열로 변환하여 약 3.5조 개의 URL을 표현할 수 있다. 읽기 중심 워크로드이므로 캐싱이 핵심이다.

---

## 8.1 문제 정의 및 요구사항

### 8.1.1 기능 요구사항

- **URL 단축**: 긴 URL을 짧은 URL로 생성
- **URL 리디렉션**: 단축 URL 접속 시 원래 URL로 리디렉션
- **예시**: 
  - 입력: `https://www.example.com/very/long/url/path?param=value`
  - 출력: `https://short.ly/abc1234`

### 8.1.2 비기능 요구사항

| 요구사항 | 값 | 설명 |
| --- | --- | --- |
| **규모** | 매일 1억 개 단축 URL 생성 | 초당 약 1,160개 생성 |
| **단축 URL 길이** | 짧을수록 좋음 | 일반적으로 7자리 이하 |
| **읽기/쓰기 비율** | 읽기 >> 쓰기 | 리디렉션 요청이 생성 요청보다 훨씬 많음 |
| **가용성** | 높음 | URL 단축기는 핵심 서비스 |

---

## 8.2 상세 설계: Base-62 변환 방식

### 8.2.1 Base-62란?

- **문자 집합**: `[0-9, a-z, A-Z]` 총 62개 문자
  - 숫자: 0-9 (10개)
  - 소문자: a-z (26개)
  - 대문자: A-Z (26개)

### 8.2.2 URL 길이 계산

- **7자리 Base-62**: 62^7 = **3,521,614,606,208** (약 3.5조 개)
- **요구사항 충족**: 매일 1억 개 × 365일 = 연간 약 365억 개 << 3.5조 개 ✅

### 8.2.3 작동 원리

1. **유일 ID 생성**: 분산 ID 생성기(예: 스노플레이크)로 숫자 ID 생성
2. **Base-62 변환**: 숫자 ID를 62진법으로 변환하여 단축 URL 생성
3. **저장**: (ID, 단축URL, 원래URL) 쌍을 DB에 저장

### 8.2.4 Base-62 인코딩/디코딩 예시

```
인코딩 예시:
ID: 1234567890
→ Base-62 변환: "1LY7VK"

디코딩 예시:
단축 URL: "1LY7VK"
→ Base-62 디코딩: 1234567890
→ DB에서 ID로 조회하여 원래 URL 반환
```

---

## 8.3 Base-62 vs 해시 기반 방식 비교

| 특징 | 해시 후 충돌 해소 | Base-62 변환 |
| --- | --- | --- |
| **단축 URL 길이** | 고정됨 (해시 길이에 따라) | ID가 커지면 길어질 수 있음 (일반적으로 7자리) |
| **유일성 보장** | 충돌 가능, 해소 전략 필요 | 유일 ID 생성기 필요, 충돌 없음 |
| **구현 복잡도** | 충돌 처리 로직 필요 | 변환 로직만 필요 (간단) |
| **보안** | 예측 불가능 (해시 특성) | ID가 순차적이면 예측 가능 (보안 취약) |
| **성능** | 해시 계산 오버헤드 | 변환 연산만 필요 (빠름) |

**선택**: Base-62 변환 방식이 구현이 간단하고 충돌이 없어 선호됩니다.

### 8.3.1 보안 개선 방안

- **문제**: 순차적 ID로 인한 URL 예측 가능
- **해결책**:
  1. ID에 랜덤 오프셋 추가
  2. Base-62 변환 전 비트 셔플링
  3. 커스텀 인코딩 테이블 사용

---

## 8.4 URL 단축 및 리디렉션 처리 흐름

### 8.4.1 URL 단축 (생성) 절차

```
1. 입력: 클라이언트로부터 긴 URL(longURL) 수신
   ↓
2. DB 확인: 해당 URL이 이미 존재하는지 검사
   ↓
3-A. 기존 반환: 존재하면 기존 단축 URL 반환
   ↓
3-B. 신규 생성:
   - 유일 ID 생성 (스노플레이크 등)
   - Base-62 변환으로 단축 URL 생성
   - (ID, 단축URL, 원래URL) DB 저장
   - 단축 URL 반환
```

**상세 단계**:
1. **입력 검증**: URL 형식 검증, 중복 체크
2. **ID 생성**: 분산 ID 생성기로 유일한 숫자 ID 생성
3. **Base-62 변환**: 숫자 ID를 62진법 문자열로 변환
4. **저장**: DB에 레코드 저장 (인덱스: 단축URL, 원래URL)
5. **반환**: 단축 URL을 클라이언트에 반환

### 8.4.2 URL 리디렉션 절차

```
1. 요청: 클라이언트가 단축 URL로 접속
   ↓
2. 캐시 확인: 캐시(Redis 등)에서 원래 URL 조회
   ↓
3-A. 캐시 히트: 원래 URL 반환 → HTTP 301/302 리디렉션
   ↓
3-B. 캐시 미스:
   - DB에서 단축 URL로 원래 URL 조회
   - 캐시에 저장 (다음 요청 대비)
   - 원래 URL 반환 → HTTP 301/302 리디렉션
```

**HTTP 리디렉션 코드**:
- **301 (Permanent Redirect)**: 영구 이동, 검색엔진이 새 URL로 인덱싱
- **302 (Temporary Redirect)**: 임시 이동, 통계 추적 용이

### 8.4.3 성능 최적화 전략

#### 캐싱 전략
- **읽기 중심**: 읽기가 쓰기보다 훨씬 많음 (예: 100:1)
- **캐시 계층**: 
  - L1: 애플리케이션 메모리 캐시 (LRU)
  - L2: Redis/Memcached (분산 캐시)
- **캐시 키**: 단축 URL
- **캐시 값**: 원래 URL
- **TTL**: 일반적으로 24시간 (또는 무제한)

#### 데이터베이스 최적화
- **인덱스**: 단축 URL에 인덱스 생성
- **파티셔닝**: 단축 URL 해시 기반 샤딩
- **읽기 복제**: 읽기 전용 복제본으로 부하 분산

---

## 8.5 시스템 아키텍처

### 8.5.1 주요 컴포넌트

1. **API 서버**: URL 단축/리디렉션 요청 처리
2. **ID 생성 서비스**: 분산 ID 생성기 (스노플레이크 등)
3. **데이터베이스**: (ID, 단축URL, 원래URL) 저장
4. **캐시**: Redis/Memcached로 원래 URL 캐싱
5. **리디렉션 서버**: 단축 URL 접속 시 원래 URL로 리디렉션

### 8.5.2 데이터 모델

```sql
-- URL 테이블
CREATE TABLE urls (
    id BIGINT PRIMARY KEY,
    short_url VARCHAR(7) UNIQUE NOT NULL,
    long_url TEXT NOT NULL,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    expires_at TIMESTAMP NULL,
    click_count BIGINT DEFAULT 0,
    INDEX idx_short_url (short_url)
);
```

### 8.5.3 확장성 고려사항

- **수평 확장**: API 서버 무상태(stateless) 설계로 로드밸런서 뒤에 여러 인스턴스 배치
- **데이터베이스 샤딩**: 단축 URL 해시 기반으로 여러 DB에 분산
- **CDN 활용**: 리디렉션 요청을 CDN으로 오프로드하여 지연 시간 감소